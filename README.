 Voice Assistant (Emotion Detection) Using Python

A smart, Python-powered voice assistant capable of detecting and responding empathetically to human emotions based on speech input. This project bridges the gap between human emotions and artificial intelligence, with potential applications in mental health, customer support, and education.


Traditional voice assistants lack emotional intelligence and fail to respond empathetically. This project addresses that limitation by integrating **emotion detection** with a voice assistant system, enhancing user interaction through emotional awareness.

---

 Objectives

 Detect human emotions (happy, sad, angry, neutral) from voice using ML.
 Generate empathetic and context-aware responses.
 Ensure real-time processing and feedback.
 Enable easy integration with other systems (IoT, apps).
 Improve accessibility for emotionally expressive users.


**Programming Language**: Python  
Libraries 
  - `speech_recognition` for speech-to-text
  - `pyttsx3` for text-to-speech
  - `librosa`, `scikit-learn`, or `keras` for emotion detection
- **Models**: RNN / Rule-Based Detection
- **Evaluation Metrics**: Accuracy, Confusion Matrix, Word Error Rate (WER), Real-Time Latency

---

 System Modules

 Module 1: Audio Feature Extraction
- Extract pitch, loudness, MFCC features
- Use RMS energy to detect emotion

 Module 2: Emotion Classification
- RNN-based classifier or rule-based threshold detection

 Module 3: Empathetic Response Generation
- Generates appropriate replies via `pyttsx3`

---

 Architecture

- 📥 Voice Input Interface
- 📊 Audio Processing Layer (feature extraction)
- 🧠 Emotion Recognition (ML model / rules)
- 🗣️ Text-to-Speech Response Generator
- 🔁 User Interaction Loop

 Project Report

📑 Download Full Report: https://pdflink.to/1285cbfe/  (report/final_report.pdf)

---

 **Emotion Classification Accuracy** using confusion matrix  
**Speech-to-Text Error Rate** using WER  
**Real-Time Processing Speed** for each module  
**RMS Energy Threshold Evaluation**

---

## 🏆 Advantages

- Real-time empathy simulation
- Improved engagement
- Personalized and adaptive responses
- Support for mental health tools and education


⚠️ Limitations

- Limited to 4 basic emotions
- May vary with accents and noise
- Requires robust preprocessing for diverse users
- Real-time performance can vary by hardware

🚀 Future Enhancements

- 🌍 Multilingual emotion recognition
- 😯 Support for complex emotions like fear/disgust
- 🧠 Context-aware responses
- 📱 Integration with mobile/wearable devices
- 🧾 Emotion detection from text (chats/emails)
- 💬 AI-based therapeutic dialogues

 🌐 Applications

- Mental Health Support Systems
- Educational Tools
- Customer Service Agents
- Elderly Care Devices
- Smart Home Assistants
- Interactive Gaming & Entertainment

 Team Members

- **Nitya** – 2022BCSE07AED958  
- **Manasvi S** – 2022BCSE07AED957  
- **Doreswamy S B** – 2022BCSE07AED956  
- **Poornima** – 2022BCSE07AED953  
- **Darshan** – 2022BCSE07AED954

**Guide:** Dr. Sridhar Devarajan

---

## 📜 References

> Refer to the project report for a list of 12+ academic and technical references.

Made by Nitya and Team | [github.com/Shettynitya](https://github.com/ Shettynitya)

